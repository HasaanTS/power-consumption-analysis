{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SyedHassaanTauqeer 19-01-2019 Final Code Notebook Draft\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_write(df, path, name):\n",
    "    #This is a simple function that writes a dataframe to a csv file\n",
    "    return df.to_csv(path+name+'.csv', sep = ';', index = False)#Making index False helps ignore index column when writing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatConv(arr):#This function takes in a python list/ numpy array and replaces any commas left in the thousand position in\n",
    "    #numerical values. This stands as a safeguard to ensure all values are float for easier future operations\n",
    "    temp = []\n",
    "    for i in range(len(arr)):\n",
    "        temp.append( float(str(arr[i]).replace(\",\", \"\") ) )\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perScale(arr):#This function is used to scale a list or numpy array to percentage ratio. \n",
    "    #This is done by taking the maximum value in that array and scaling every other value relatively. No trailing decimal places\n",
    "    #left for the sake of ease in matching and querying at future stages\n",
    "    scaled = []\n",
    "    tempMax = max(arr)\n",
    "    for i in range(len(arr)):\n",
    "        scaled.append( round(float(arr[i]/tempMax)*100, 0) )\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minFil(arrList, win):#This function takes in a list/numpy array and applies a rolling window operation to get the minimum \n",
    "    #value within that window. \n",
    "    tempDiff = arrList\n",
    "    appVal = arrList[len(arrList)-1]# To ensure the input and output lengths of the array are same, the last value of the input \n",
    "    #array is repeated the number of times that matches one less than the window size\n",
    "    tempDiff = np.insert(tempDiff, len(tempDiff), np.repeat(appVal, win-1))\n",
    "\n",
    "    arrListDF = pd.DataFrame({\"arr\":tempDiff})#since the rolling function in pandas is really efficient, we convert the array\n",
    "    #to Series data\n",
    "    tempDiff = arrListDF.rolling(win).min().dropna()#all NaN values are dropped to not cause issues in future operations\n",
    "    tempDiff = tempDiff['arr'].values\n",
    "    \n",
    "    return tempDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diffIndexMaker(arr):#This function takes in an numpy array/list and computes the difference in between each consecutive value\n",
    "    #To ensure that the input and output lengths of the array remain same, the first value of the array is pre-pended.\n",
    "    #This doesn't affect the computation or any future operations but keeps length consistent\n",
    "    temp = arr.item(0)\n",
    "    arr = np.insert(arr, 0, temp)#pre-pending the first value to the array\n",
    "    return np.diff(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This block makes all the data files uniform in shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that is re-structured by the following block is then taken to label the \"spin phases\" and the \n",
    "\"end spin phases\". The labeled files are copies of this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change working directory to raw data folder\n",
    "os.chdir('C:\\\\Users\\\\Labyrinth\\\\JUPYTER NOTEBOOKS\\\\WeWash_Praktikum_TUM3sem\\\\WeWash_Analysis_ver2\\\\Data\\\\raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machines:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machines:  150\n",
      "machines:  146\n",
      "machines:  400\n"
     ]
    }
   ],
   "source": [
    "for _file in glob.glob(\"*.csv\"): #Iterating through all files in the raw data directory\n",
    "    writingDF = pd.DataFrame(columns=['machine', 'pow', 'time', 'timeStamp'])\n",
    "    labtsDF = pd.read_csv(_file, delimiter=',')\n",
    "    \n",
    "    machines = labtsDF['reservation_id'].unique() #Uniques machines are filtered based on id\n",
    "    print 'machines: ', len(machines)\n",
    "    for i in range(len(machines)):#Each of the machines has multiple records. \n",
    "        tempDF = labtsDF[labtsDF['reservation_id'] == machines[i]]\n",
    "        subSet = tempDF[['reservation_id', 'power', 'sample_time']] #Subset the dataframe on relevant columns\n",
    "        subSet['time'] = np.arange(len(subSet['power'].values))# Making time stamps uniform. This is just an incremental order \n",
    "        #given to timestamps. e.d 0,1,2,3,4 instead of UNIX timestamps which compromises uniformity\n",
    "        subSet.rename(index=str, columns={\"reservation_id\": \"machine\", \"power\": \"pow\", \"sample_time\":\"timeStamp\"}, inplace=True)\n",
    "        #The headers are re-named to match the writing data frame for seamless stitching of records\n",
    "        writingDF = writingDF.append(subSet)\n",
    "\n",
    "    savingPath = 'C:\\\\Users\\\\Labyrinth\\\\JUPYTER NOTEBOOKS\\\\WeWash_Praktikum_TUM3sem\\\\WeWash_Analysis_ver2\\\\Data\\\\intermediate\\\\'\n",
    "    fileName = 'uniform_'+str(len(machines))+'_'+str(machines[0])+'-'+str(machines[len(machines)-1])\n",
    "    file_write(writingDF, path= savingPath, name= fileName) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part prepares the Labelled data for the transformation to the training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change working directory to labelled training data folder\n",
    "os.chdir('C:\\\\Users\\\\Labyrinth\\\\JUPYTER NOTEBOOKS\\\\WeWash_Praktikum_TUM3sem\\\\WeWash_Analysis_ver2\\\\Data\\\\intermediate\\\\labelled\\\\train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machines:  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Labyrinth\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machines:  146\n",
      "machines:  150\n",
      "machines:  282\n",
      "696\n",
      "   end_spin  machine    pow  powMin  scaledPowDiff  scaledPowMin  scaledPower  \\\n",
      "0       NaN   211286  0.094   0.086            0.0           0.0          0.0   \n",
      "1       NaN   211286  0.090   0.082            0.0           0.0          0.0   \n",
      "2       NaN   211286  0.086   0.082            0.0           0.0          0.0   \n",
      "3       NaN   211286  0.086   0.082            0.0           0.0          0.0   \n",
      "4       NaN   211286  0.086   0.074            0.0           0.0          0.0   \n",
      "\n",
      "   scaledTime  spins  time  \n",
      "0         0.0    NaN   0.0  \n",
      "1         0.0    NaN   1.0  \n",
      "2         0.0    NaN   2.0  \n",
      "3         0.0    NaN   3.0  \n",
      "4         1.0    NaN   4.0  \n"
     ]
    }
   ],
   "source": [
    "bigDF = pd.DataFrame() #This is the master dataframe which will contain all the training data. In this case we shall have\n",
    "#data for about 146+150+400 = 900 Machines. Individual records are definitely way more than that.\n",
    "for _file in glob.glob(\"*.csv\"): #Iterating through all files in the directory\n",
    "    labtsDF = pd.read_csv(_file, delimiter=';')\n",
    "    machines = labtsDF['machine'].unique() #Uniques machines are filtered based on id\n",
    "    print 'machines: ', len(machines)\n",
    "    \n",
    "    for i in range(len(machines)):#Each of the machines has multiple records. \n",
    "        tempDF = labtsDF[labtsDF['machine'] == machines[i]]#now that we have a subset of the dataframe filtered on \n",
    "        #the machine, we'll add features to it and then append it to the main dataframe.\n",
    "        tempDF['pow'] = floatConv(tempDF['pow'].values)#float conversions\n",
    "        tempDF['time'] = floatConv(tempDF['time'].values)#float conversions\n",
    "        tempDF['scaledPower'] = perScale(tempDF['pow'].values)#percentage scaling\n",
    "        tempDF['scaledTime'] =  perScale(tempDF['time'].values)#percentage scaling\n",
    "        tempDF['powMin'] = minFil(tempDF['pow'].values, win=5)#applying minimum filter to main power values\n",
    "        tempDF['scaledPowMin'] = minFil(tempDF['scaledPower'].values, win=5)#applying minimum filter to scaled power values\n",
    "        tempDF['scaledPowDiff'] = diffIndexMaker(tempDF['scaledPower'].values)#creating the difference index from scaled power\n",
    "        bigDF = bigDF.append(tempDF)\n",
    "        \n",
    "print len(bigDF['machine'].unique())\n",
    "print bigDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It is extremely important to fill all NaN values because otherwise it will hamper the training process\n",
    "bigDF['end_spin'] = bigDF['end_spin'].fillna(0.0)\n",
    "bigDF['spins'] = bigDF['spins'].fillna(0.0)\n",
    "bigDF['pow'] = bigDF['pow'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Time to finally write the training dataframe to a csv file\n",
    "savingPath = 'C:\\\\Users\\\\Labyrinth\\\\JUPYTER NOTEBOOKS\\\\WeWash_Praktikum_TUM3sem\\\\WeWash_Analysis_ver2\\\\Data\\\\processed\\\\' \n",
    "fileName = 'UniFeatScaledV2_146-150-400_696_128895-212236'\n",
    "file_write(bigDF, path=savingPath, name=fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
