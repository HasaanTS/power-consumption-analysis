{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SyedHassaanTauqeer 19-01-2019 Final Code Notebook Draft \n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import datetime\n",
    "import sklearn.cluster as skc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('fast')\n",
    "import statsmodels.api as sm\n",
    "import operator\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['text.color'] = 'k'\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_write(df, path, name):\n",
    "    #This is a simple function that writes a dataframe to a csv file\n",
    "    return df.to_csv(path+name+'.csv', sep = ';', index = False)#Making index False helps ignore index column when writing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatConv(arr):#This function takes in a python list/ numpy array and replaces any commas left in the thousand position in\n",
    "    #numerical values. This stands as a safeguard to ensure all values are float for easier future operations\n",
    "    temp = []\n",
    "    for i in range(len(arr)):\n",
    "        temp.append( float(str(arr[i]).replace(\",\", \"\") ) )\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perScale(arr):#This function is used to scale a list or numpy array to percentage ratio. \n",
    "    #This is done by taking the maximum value in that array and scaling every other value relatively. No trailing decimal places\n",
    "    #left for the sake of ease in matching and querying at future stages\n",
    "    scaled = []\n",
    "    tempMax = max(arr)\n",
    "    for i in range(len(arr)):\n",
    "        scaled.append( round(float(arr[i]/tempMax)*100, 0) )\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minFil(arrList, win):#This function takes in a list/numpy array and applies a rolling window operation to get the minimum \n",
    "    #value within that window. \n",
    "    tempDiff = arrList\n",
    "    appVal = arrList[len(arrList)-1]# To ensure the input and output lengths of the array are same, the last value of the input \n",
    "    #array is repeated the number of times that matches one less than the window size\n",
    "    tempDiff = np.insert(tempDiff, len(tempDiff), np.repeat(appVal, win-1))\n",
    "\n",
    "    arrListDF = pd.DataFrame({\"arr\":tempDiff})#since the rolling function in pandas is really efficient, we convert the array\n",
    "    #to Series data\n",
    "    tempDiff = arrListDF.rolling(win).min().dropna()#all NaN values are dropped to not cause issues in future operations\n",
    "    tempDiff = tempDiff['arr'].values\n",
    "    \n",
    "    return tempDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diffIndexMaker(arr):#This function takes in an numpy array/list and computes the difference in between each consecutive value\n",
    "    #To ensure that the input and output lengths of the array remain same, the first value of the array is pre-pended.\n",
    "    #This doesn't affect the computation or any future operations but keeps length consistent\n",
    "    temp = arr[0]#.item(0)\n",
    "    arr = np.insert(arr, 0, temp)#pre-pending the first value to the array\n",
    "    return np.diff(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TestPairGen(df):#This function follows the exact same pattern as the training pair generator function\n",
    "    #It just requires the testing dataframe instead and does not produce a separate label array from the testing pair\n",
    "    pair = []\n",
    "    for index, rows in df.iterrows():\n",
    "        pair.append( ( rows['scaledTime'].astype(float), rows['scaledPower'].astype(float), rows['scaledPowMin'].astype(float), rows['scaledPowDiff'] ) )\n",
    "\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thresher(DF):#This is the most important function for the heat phase detection\n",
    "    #It works on the assumption that the incoming dataframe has three main parts (heat, spin+end and noise)\n",
    "    #The noise is the most occuring element and becomes an easy target for elimination\n",
    "    #The spin+endspin is seperated by the sureity that the end always exists in the last time stamps\n",
    "    #Whatever is left is thus the heating phase\n",
    "    df = pd.DataFrame({\"p\": DF['pow'], \"time\": np.arange(len(DF['pow'].values))})\n",
    "    x_tag = df['time'].values\n",
    "\n",
    "    print \"length: \", len(df)\n",
    "\n",
    "    X = df['p'].values.reshape(-1, 1)#single feature re-shaping for cluster prediction - KMeans requires this shape\n",
    "    model = skc.KMeans(n_clusters=3)#Base assumption that data always has the 3 parts(heat, spin+end and noise)\n",
    "    y_pred = model.fit_predict(X)\n",
    "\n",
    "    #dictionary structure to store all predicted cluster labels against respective time stamps\n",
    "    refTsPowDict = collections.OrderedDict()\n",
    "    \n",
    "    for i in range(len(x_tag)):\n",
    "        refTsPowDict[x_tag[i]] = y_pred[i] #populating dictionary\n",
    "    \n",
    "    clustCount = collections.Counter(y_pred)#counting labels to see which labels has how many points\n",
    "    #this is done because the segments are not always uniformly labelled\n",
    "    #e.g heat phase might get label '1' in one case and the very next machine iteration may give heat phase a label of '0'\n",
    "    looper = len(x_tag)\n",
    "    for  i in range(looper): #deleting noise\n",
    "        if refTsPowDict[x_tag[i]] == keywithmaxval(clustCount):#gets the cluster key with most frequent value(noise timestamps)\n",
    "            del refTsPowDict[x_tag[i]]#delete dictionary entry, theeby deleting time stamps of noise\n",
    "    \n",
    "    last_key = refTsPowDict.keys()[-1]#possible end spin. Gives the very last time stamp which has to be end spin in theory. \n",
    "    H_timeArr = [] #time stamps for heating phase\n",
    "    comb_timeArr = [] #time stamps for last phases\n",
    "    for key, value in refTsPowDict.items():\n",
    "        if value  ==  refTsPowDict[last_key]:#query all dict on this value to get keys for this cluster\n",
    "            comb_timeArr.append(key)#if query matches then it is from the spin+end group\n",
    "        else:\n",
    "            H_timeArr.append(key)#otherwise it's heat\n",
    "            \n",
    "    p_val = []\n",
    "    for i in range(len(H_timeArr)): #create and return dataframe for heat phase\n",
    "        p_val.append(df[df['time'] == H_timeArr[i]]['p'].values[0])\n",
    "    hDF = pd.DataFrame({'time': H_timeArr, 'pow':p_val})\n",
    "    \n",
    "    return hDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywithmaxval(d): #This function just takes in a dictionary and returns key that corresponds to the maximum value among \n",
    "    #the dictionary values\n",
    "    #for example {A:2, B:6} then the function returns B\n",
    "    v=list(d.values())\n",
    "    k=list(d.keys())\n",
    "    return k[v.index(max(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarySegmenter(arr):# This function works with the \"Segmenter\" function. It's function is to create an array which contains \n",
    "    #the time stamps where a positive(where value is 1) prediction occurs. It is used mainly by the predicted vectors. \n",
    "    flag = 1#positive prediction\n",
    "    Seg = []\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i]==flag: #phase started\n",
    "            Seg.append(i)\n",
    "    return np.asarray(Seg)\n",
    "\n",
    "\n",
    "def Segmenter(Seg): #This function is very important to find the togetherness of the clustered time stamps and at the same time\n",
    "    #break up a single segment(timestamp array) which has a huge difference between it's values into separate segments\n",
    "    #for example a segment/timestamp array is as follows {12, 13,15, 18, 55, 60, 66, 69}\n",
    "    #this array definitely needs to be broken up in to {12, 13,15, 18} and {55, 60, 66, 69}\n",
    "    #to segment the labelled predictions. Usable for spins & end spin.\n",
    "    \n",
    "    #We also need to see if there are a lot of confident points nearby or if there is a sporadic scatter of timestamps.\n",
    "    #in this case we need to get rid of them because they will act as noise and result in un-confident labels being considered\n",
    "    #as actual labels.\n",
    "    threshold = 3 #minimum number of points in a segment to qualify as a confident prediction/group\n",
    "    \n",
    "    # deciding metric to  find breakers in sequence\n",
    "    #https://www.researchgate.net/post/Can_anybody_suggests_me_how_to_split_a_continuous_variable_into_high_and_low_value_group_for_testing_its_moderating_effect\n",
    "\n",
    "    diffArr = diffIndexMaker(Seg)#gets the differences between the timestamps\n",
    "    iqr = np.percentile(diffArr, 75) - np.percentile(diffArr, 25)#inter-quartile range\n",
    "\n",
    "    count = 0#counts segments\n",
    "    breakers = []#contains the index of the timestamp where the break occurs\n",
    "\n",
    "    buff = {}#dictionary that contains the \n",
    "    prev = 0 #last marker for subsetting\n",
    "    for i in range(len(diffArr)):\n",
    "\n",
    "        if Seg[i] == 0:#only usable in the case of heat phase\n",
    "            #if the timestamps carry a '0' point that means the heat phase was started at time 't0', which is not\n",
    "            #possible because a cycle never starts from heat phase on 't0'. This case is only possible if the previous \n",
    "            #wash cycle was not completed and carried up in this cycle. We just consider it as an invalid cycle.\n",
    "            print 'INVALID CYCLE - Previous continuation' \n",
    "\n",
    "        else:\n",
    "            temp = float(diffArr[i])/float(Seg[i])*100\n",
    "    \n",
    "            if temp >(float(np.median(diffArr)) + float(iqr)):#filtering on median and inter-quartile range \n",
    "            #since data is assumed to be non-centered and un-symmetric\n",
    "\n",
    "                breakers.append((diffArr[i], i))\n",
    "                count +=1\n",
    "            count += 1#this is the number of segments  ---|---|--- in this case breakers=2 and segments=3\n",
    "    if not breakers: #only one segment\n",
    "\n",
    "        buff[0]  = Seg[prev:len(Seg)]\n",
    "\n",
    "    if not (not (breakers)):#ensures that there is at least one breaker which means two segments        \n",
    "        for i in range(len(breakers)):\n",
    "            buff[i] = Seg[prev:breakers[i][1]]\n",
    "            \n",
    "            if len(Seg[prev:breakers[i][1]])<threshold:#for entries less than thresh utilize the loop\n",
    "                del buff[i]\n",
    "                \n",
    "            prev = breakers[i][1]\n",
    "               \n",
    "        buff[len(breakers)] = Seg[prev:len(Seg)]# last segment added manually\n",
    "        \n",
    "        if len(Seg[prev:len(Seg)])<threshold:#for entries less than thresh utilize the loop\n",
    "            del buff[len(breakers)] #entries in segment less confidently predicted are deleted\n",
    "\n",
    "            # e.g 3 points in a prediction segment are at least required for it to be labeled a separate segment\n",
    "\n",
    "    return Seg, buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gapFinder(_dict, thresh):#This function finds gaps between dictionary entries of segments, which are basically timestamps.\n",
    "    #If the gaps are big enough it lets them be otherwise it fuses them together to make a shorter dictioanry\n",
    "    #It works sort of in the opposite manner as the Segmenter function\n",
    "    #e.g {A:[1,2,4,5,6], B:[10,12,14,17], C:[50,55,59,62]} will become {A:[1,2,4,5,6,10,12,14,17], C:[50,55,59,62]}\n",
    "    k = _dict.keys()\n",
    "\n",
    "    if len(k) ==  1: # first case no pre return the only time markers\n",
    "        return _dict\n",
    "    else:\n",
    "        newArr = []\n",
    "        newDict  = collections.OrderedDict()\n",
    "        firstPhase = _dict[k[0]] #load initial dictionary\n",
    "\n",
    "        for i in range(len(k)):\n",
    "            if i == 0: continue\n",
    "\n",
    "            tempPrev = firstPhase\n",
    "            lastPrev  = firstPhase[len(firstPhase)-1]\n",
    "            tempCurrent = _dict[k[i]]\n",
    "            firstCurrent = tempCurrent[0]\n",
    "            gap = firstCurrent  - lastPrev\n",
    "            # the gap has to be big enough relative to the last point in the previous array\n",
    "            if ( float(gap)/float(lastPrev) )  < thresh:#acceptible diff means fuse this value with previous and delete this entry\n",
    "\n",
    "                newArr = np.concatenate((tempPrev, tempCurrent),axis=None)\n",
    "\n",
    "                firstPhase = newArr #set this as previous\n",
    "\n",
    "         \n",
    "            else:\n",
    "                newDict[i] = tempPrev\n",
    "                firstPhase = tempCurrent\n",
    "                \n",
    "        newDict[k[len(k)-1]+1] = newArr\n",
    "        \n",
    "        return newDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longestRecKey(_dict):# This function just finds the longest valued array in a dictionary and returns that value array\n",
    "    _valLen = []\n",
    "    k = _dict.keys()\n",
    "    v = _dict.values()\n",
    "    for items in v: _valLen.append(len(items)) \n",
    "    return  _dict[k[_valLen.index(max(_valLen))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preFunc(_dict):#This function analyses the possiblity of a pre-wash cycle existing, in case it does it identifies\n",
    "    #the markers of the phase\n",
    "    #Along that it also identifies the main heating phase timestamp array\n",
    "    _dict = gapFinder(_dict, 0.5)\n",
    "    preStart, preEnd = 0, 0\n",
    "    preFlag = False\n",
    "    k = _dict.keys()\n",
    "    if len(k) ==  1: # first case no pre return the only time markers\n",
    "        return preFlag, preStart, preEnd, _dict[k[0]]\n",
    "    else:\n",
    "        firstPhase = _dict[k[0]] #load initial dictionary\n",
    "        longestPhase = longestRecKey(_dict)\n",
    "        \n",
    "        if np.array_equal(firstPhase, longestPhase):\n",
    "            return preFlag, preStart, preEnd, longestPhase\n",
    "        \n",
    "        elif _dict.values().index(longestPhase) > _dict.values().index(firstPhase):\n",
    "            preFlag = True\n",
    "            preStart = firstPhase[0]\n",
    "            preEnd  = firstPhase[len(firstPhase)-1]\n",
    "            return preFlag, preStart, preEnd, longestPhase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def heatAnalysis(_dict, df):#This function takes in the heating phase dataframe that comes after the cluster(KMeans)processing\n",
    "    #and the dictionary that comes after the segmenting of the heating phase\n",
    "    #This function provides the markers for the heating phase and the energy calculation by integration to stay precise\n",
    "    Phase_start, Phase_end = 0, 0 \n",
    "    Energy = 0\n",
    "\n",
    "    preFlag, preStart, preEnd, timeArr = preFunc(_dict)\n",
    "\n",
    "    Phase_start = timeArr[0]\n",
    "    Phase_end  = timeArr[len(timeArr)-1]\n",
    "\n",
    "    plt.axvline(x=Phase_start, color='r')\n",
    "    plt.axvline(x=Phase_end, color='r')\n",
    "\n",
    "    #The energy is calculated by a simpole formula\n",
    "    #The power for each of the heat phase timestamps is added\n",
    "    #The sum is multiplied by 4 because the timestamps have an average difference of 4\n",
    "    #and divided by 3600 to convert seconds to hours for the energy to be in Watt Hour.\n",
    "    Energy = float(np.sum(df[df['time'].isin(timeArr)]['pow'].values)) * (float(4)/float(3600))\n",
    "\n",
    "    return Energy, Phase_start, Phase_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def endAnalysis(_dict, df):#This function analyses the main dataframe for end cycle information.\n",
    "    #It also takes in the end cycle prediction vector that has been segmented by the Segmenter function\n",
    "    #generally assuming that even if more than one end spin detections then the longest one is relevant\n",
    "    Phase_start, Phase_end = 0, 0\n",
    "    Energy = 0\n",
    "    timeArr = longestRecKey(_dict)\n",
    "    Phase_start = timeArr[0]\n",
    "    Phase_end  = timeArr[len(timeArr)-1]\n",
    "\n",
    "    plt.axvline(x=Phase_start, color='g')\n",
    "    plt.axvline(x=Phase_end, color='g')\n",
    "    \n",
    "    #The energy is calculated on the same principle as the heat phase\n",
    "    #Each 4 second server ping is converted to hours by division by 3600\n",
    "    #This is then multiplied to the sum of the end phase power sum to give the energy in Watt Hour\n",
    "    Energy = float(np.sum(df[df['time'].isin(timeArr)]['pow'].values)) * (float(4)/float(3600))\n",
    "\n",
    "    return Energy, Phase_start, Phase_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spinAnalysis(_dict, df):#This function analyses the spins in a single cycle. It requires the main dataframe and the\n",
    "    #dictionary that results from the segmentation of the spin predicted vector.\n",
    "    k = _dict.keys()\n",
    "    Energies = []\n",
    "    phaseStarts = []#all spin phase starts are stored here\n",
    "    phaseEnds = []#all spin end phases are stored here\n",
    "    spins = len(k)#gives the number of spins\n",
    "\n",
    "    for i in range(spins):\n",
    "        Energy = 0\n",
    "        timeArr = _dict[k[i]]\n",
    "\n",
    "        Phase_start = timeArr[0]\n",
    "        Phase_end  = timeArr[len(timeArr)-1]\n",
    "        phaseStarts.append(Phase_start)\n",
    "        phaseEnds.append(Phase_end)\n",
    "\n",
    "        plt.axvline(x=Phase_start, color='b')\n",
    "        plt.axvline(x=Phase_end, color='b')\n",
    "        \n",
    "        Energy = float(np.sum(df[df['time'].isin(timeArr)]['pow'].values)) * (float(4)/float(3600))\n",
    "        Energies.append(Energy)\n",
    "    #It returns the number of spins, their markers in two lists and their mean energies if more than one spin exist\n",
    "    return spins, phaseStarts, phaseEnds, np.mean(Energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase to begin from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test dataframe is read into the notebook.\n",
    "The models for spin and end spin detection are then loaded in.\n",
    "A JSON file composed of the entire analysis result is then generated after the analysis runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine</th>\n",
       "      <th>pow</th>\n",
       "      <th>time</th>\n",
       "      <th>spins</th>\n",
       "      <th>end_spin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143200</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143200</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   machine   pow  time  spins  end_spin\n",
       "0   143200  0.07     0    NaN       NaN\n",
       "1   143200  0.06     1    NaN       NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_file = 'C:\\\\Users\\\\Labyrinth\\\\JUPYTER NOTEBOOKS\\\\WeWash_Praktikum_TUM3sem\\WeWash_Analysis_ver2\\\\Data\\\\intermediate\\\\labelled\\\\test\\\\UniLabeled_data_50_143200-143324.csv'\n",
    "labtsDF = pd.read_csv(_file, delimiter=';')\n",
    "labtsDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machines:  50\n"
     ]
    }
   ],
   "source": [
    "machines = labtsDF['machine'].unique() #Uniques machines are filtered based on id\n",
    "print 'machines: ', len(machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine</th>\n",
       "      <th>pow</th>\n",
       "      <th>time</th>\n",
       "      <th>spins</th>\n",
       "      <th>end_spin</th>\n",
       "      <th>scaledPower</th>\n",
       "      <th>scaledTime</th>\n",
       "      <th>powMin</th>\n",
       "      <th>scaledPowMin</th>\n",
       "      <th>scaledPowDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143200</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143200</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   machine   pow  time  spins  end_spin  scaledPower  scaledTime  powMin  \\\n",
       "0   143200  0.07   0.0    NaN       NaN          0.0         0.0    0.06   \n",
       "1   143200  0.06   1.0    NaN       NaN          0.0         0.0    0.06   \n",
       "\n",
       "   scaledPowMin  scaledPowDiff  \n",
       "0           0.0            0.0  \n",
       "1           0.0            0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = pd.DataFrame()\n",
    "for i in range(len(machines)):#Each of the machines has multiple records. \n",
    "    tempDF = labtsDF[labtsDF['machine'] == machines[i]]#now that we have a subset of the dataframe filtered on \n",
    "    #the machine, we'll add features to it and then append it to the test dataframe.\n",
    "    tempDF['pow'] = floatConv(tempDF['pow'].values)#float conversions\n",
    "    tempDF['time'] = floatConv(tempDF['time'].values)#float conversions\n",
    "    tempDF['scaledPower'] = perScale(tempDF['pow'].values)#percentage scaling\n",
    "    tempDF['scaledTime'] =  perScale(tempDF['time'].values)#percentage scaling\n",
    "    tempDF['powMin'] = minFil(tempDF['pow'].values, win=5)#applying minimum filter to main power values\n",
    "    tempDF['scaledPowMin'] = minFil(tempDF['scaledPower'].values, win=5)#applying minimum filter to scaled power values\n",
    "    tempDF['scaledPowDiff'] = diffIndexMaker(tempDF['scaledPower'].values)#creating the difference index from scaled power\n",
    "    testDF = testDF.append(tempDF)\n",
    "testDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It is extremely important to fill all NaN values because otherwise it will hamper the testing process\n",
    "testDF['end_spin'] = testDF['end_spin'].fillna(0.0)\n",
    "testDF['spins'] = testDF['spins'].fillna(0.0)\n",
    "testDF['pow'] = testDF['pow'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the SVM classifiers\n",
    "_filePath = 'C:\\\\Users\\\\Labyrinth\\\\JUPYTER NOTEBOOKS\\\\WeWash_Praktikum_TUM3sem\\\\WeWash_Analysis_ver2\\\\Models\\\\50_146_4f\\\\'\n",
    "loaded_spinModel = pickle.load(open(_filePath+'spinSVM_50_146_4f.sav', 'rb'))\n",
    "loaded_endModel = pickle.load(open(_filePath+'endSVM_50_146_4f.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine# 143200\n",
      "length:  1014\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-62a655b4e23f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m#4 segment spins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mtempLab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtempBuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSegmenter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinarySegmenter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTestPred_spin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mspinDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-37a4f24bdad9>\u001b[0m in \u001b[0;36mSegmenter\u001b[1;34m(Seg)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#https://www.researchgate.net/post/Can_anybody_suggests_me_how_to_split_a_continuous_variable_into_high_and_low_value_group_for_testing_its_moderating_effect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mdiffArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiffIndexMaker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#gets the differences between the timestamps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0miqr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#inter-quartile range\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-08385766f6f6>\u001b[0m in \u001b[0;36mdiffIndexMaker\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#To ensure that the input and output lengths of the array remain same, the first value of the array is pre-pended.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#This doesn't affect the computation or any future operations but keeps length consistent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#.item(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#pre-pending the first value to the array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne6HXSAO+qgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe6b7f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jsonDF  = pd.DataFrame(columns=['mac', 'pre', 'heat', 'spin', 'end', 'avgCycleEnergy'])\n",
    "\n",
    "for i in range(len(machines)):#THe analysis is run on each machine sample individually\n",
    "    testSampleDF = testDF[testDF['machine'] == machines[i]]\n",
    "\n",
    "    print 'machine#', machines[i]\n",
    "    sample = np.asarray(TestPairGen(testSampleDF))\n",
    "#     print 'samp: ', sample\n",
    "    TestPred_spin = loaded_spinModel.predict(sample)\n",
    "    TestPred_end = loaded_endModel.predict(sample)\n",
    " \n",
    "    \n",
    "    #1 Check for end\n",
    "    if np.count_nonzero(TestPred_end)>1:\n",
    "    #1.1 if not end then invalid cycle\n",
    "        tempLab, tempBuff = Segmenter(binarySegmenter(TestPred_end))\n",
    "        \n",
    "#         #2 If valid then run end analysis\n",
    "        endDict = {}\n",
    "        endEnergy, end_start, end_end = endAnalysis(tempBuff, testSampleDF)\n",
    "        endDict['endEnergy'] = endEnergy\n",
    "        endDict['end_start'] = end_start\n",
    "        endDict['end_end'] = end_end\n",
    "    \n",
    "        #2.1  segment hDF & run analysis\n",
    "        hDF = thresher(testSampleDF)\n",
    "        tempLab, tempBuff = Segmenter(hDF['time'].values)\n",
    "        #2.2 check pre  && get markers\n",
    "        preDict = collections.OrderedDict()\n",
    "        prePhaseFlag, pre_start, pre_end, longestPhase = preFunc(tempBuff)\n",
    "        preDict['preFlag'] = prePhaseFlag\n",
    "        preDict['pre_start'] = pre_start\n",
    "        preDict['pre_end'] = pre_end\n",
    "            \n",
    "        #3Run heat analysis\n",
    "        heatDict = {}\n",
    "        heatEnergy, heat_start, heat_end = heatAnalysis(tempBuff, hDF)\n",
    "        heatDict['heatEnergy'] = heatEnergy\n",
    "        heatDict['heat_start'] = heat_start\n",
    "        heatDict['heat_end'] = heat_end\n",
    "        \n",
    "\n",
    "        #4 segment spins\n",
    "        tempLab, tempBuff = Segmenter(binarySegmenter(TestPred_spin))\n",
    "        \n",
    "        spinDict = {}\n",
    "        spinNo, spin_start, spin_end, avgSpinEnergy = spinAnalysis(tempBuff, testSampleDF)\n",
    "        spinDict['spins'] = spinNo\n",
    "        spinDict['avgEnergy'] = avgSpinEnergy\n",
    "        for j in range(spinNo):\n",
    "            spinDict[j+1] = (spin_start[j],  spin_end[j])\n",
    "            \n",
    "        #4.1 plot all markers and sample\n",
    "        plt.plot(np.arange(len(testSampleDF['powMin'].values)), testSampleDF['powMin'].values)\n",
    "\n",
    "        \n",
    "        #5 Avg cycle energy\n",
    "        avgCycEnergy = np.mean(testSampleDF['pow'].values)\n",
    "        print 'avg energy for cycle: ', avgCycEnergy \n",
    "        \n",
    "        #6 JSON export\n",
    "        record = []\n",
    "        record.append(machines[i])\n",
    "        record.append(preDict)#pre\n",
    "        record.append(heatDict)#heat\n",
    "        record.append(spinDict)#spin\n",
    "        record.append(endDict)#end\n",
    "        record.append(avgCycEnergy)#avg cycle energy\n",
    "        jsonDF.loc[len(jsonDF)] = record\n",
    "\n",
    "        \n",
    "    plt.show()\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
